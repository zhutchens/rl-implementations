{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaae308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as opt\n",
    "import random\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch \n",
    "import gymnasium as gym\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9183fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, hidden_size: int):\n",
    "        super(Actor, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, states: torch.tensor) -> torch.tensor:\n",
    "        logits = self.layers(states)\n",
    "        return Categorical(logits = logits)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,  in_features: int, out_features: int, hidden_size: int):\n",
    "        super(Critic, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, states: torch.tensor) -> torch.tensor:\n",
    "        return self.layers(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f132de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, actor: Actor, critic: Critic, epsilon: float, gamma: float, lam: float, actor_lr: float, critic_lr: float):\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_opt = opt.Adam(self.actor.parameters(), actor_lr)\n",
    "        self.critic_opt = opt.Adam(self.critic.parameters(), critic_lr)\n",
    "\n",
    "    def save_agent(self, path: str = 'cartpole_agent.pt') -> None:\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'actor_lr': self.actor_lr,\n",
    "                'critic_lr': self.critic_lr,\n",
    "                'epsilon': self.epsilon,\n",
    "                'gamma': self.gamma,\n",
    "                'lam': self.lam,\n",
    "                'actor_in_feats': self.actor.in_features,\n",
    "                'actor_out_feats': self.actor.out_features,\n",
    "                'critic_in_feats': self.critic.in_features,\n",
    "                'critic_out_feats': self.critic.out_features,\n",
    "                'actor_hidden_size': self.actor.hidden_size,\n",
    "                'critic_hidden_size': self.critic.hidden_size,\n",
    "            }\n",
    "        }, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_agent(env, path: str, device: str) -> 'Agent':\n",
    "        chckpt = torch.load(path)\n",
    "\n",
    "        actor = Actor(chckpt['hyperparameters']['actor_in_feats'], chckpt['hyperparameters']['actor_out_feats'], chckpt['hyperparameters']['actor_hidden_size']).to(device)\n",
    "        critic = Critic(chckpt['hyperparameters']['critic_in_feats'], chckpt['hyperparameters']['critic_out_feats'], chckpt['hyperparameters']['critic_hidden_size']).to(device)\n",
    "        actor.load_state_dict(chckpt['actor'])\n",
    "        critic.load_state_dict(chckpt['critic'])\n",
    "\n",
    "        return Agent(\n",
    "            env, \n",
    "            actor, \n",
    "            critic, \n",
    "            chckpt['hyperparameters']['epsilon'],\n",
    "            chckpt['hyperparameters']['gamma'],\n",
    "            chckpt['hyperparameters']['lam'],\n",
    "            chckpt['hyperparameters']['actor_lr'],\n",
    "            chckpt['hyperparameters']['critic_lr']\n",
    "        )\n",
    "\n",
    "\n",
    "    def select_action(self, states: torch.tensor) -> tuple:\n",
    "        dist = self.actor.forward(states)\n",
    "        actions = dist.sample()\n",
    "\n",
    "        return actions, dist.log_prob(actions)  \n",
    "    \n",
    "    def get_state_values(self, states: torch.tensor) -> torch.tensor:\n",
    "        return self.critic.forward(states)\n",
    "\n",
    "    def update_nets(self, actor_loss: torch.tensor, critic_loss: torch.tensor) -> None:\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "\n",
    "    def fit(self, train_iters: int, timesteps: int, K: int, bs: int) -> None:\n",
    "        if bs > self.env.num_envs * timesteps:\n",
    "            raise ValueError('batch size cannot be greater than number of environments * timesteps')\n",
    "        \n",
    "        self.all_rewards = []\n",
    "        self.all_steps = []\n",
    "\n",
    "        for train_iter in range(train_iters):\n",
    "            rollout = []\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            states, _ = self.env.reset()\n",
    "            states = torch.from_numpy(states).to(device)\n",
    "\n",
    "            for _ in range(timesteps):\n",
    "                actions, probs = self.select_action(states)\n",
    "                state_vals = self.get_state_values(states)\n",
    "\n",
    "                next_states, rewards, dones, terminated, _ = self.env.step(actions.detach().cpu().numpy())\n",
    "                next_states = torch.from_numpy(next_states).to(device)\n",
    "                rewards = torch.from_numpy(rewards)\n",
    "                dones = torch.from_numpy(dones)\n",
    "\n",
    "                ep_reward += sum(rewards).item()\n",
    "\n",
    "                next_state_vals = self.get_state_values(next_states)\n",
    "                for s, sv, ns, nsv, a, p, r, d in zip(states, state_vals.detach(), next_states, next_state_vals, actions, probs.detach(), rewards, dones):\n",
    "                    rollout.append([s, sv, ns, nsv, a, p, r, d])\n",
    "\n",
    "                states = next_states\n",
    "                ep_steps += 1\n",
    "            \n",
    "            print('finished episode:', train_iter)\n",
    "            print('total reward:', ep_reward)\n",
    "            print('number of steps:', ep_steps)\n",
    "            print('-' * 15)\n",
    "\n",
    "            self.all_rewards.append(ep_reward)\n",
    "            self.all_steps.append(ep_steps)\n",
    "\n",
    "            next_advantage = 0\n",
    "            for t in reversed(range(len(rollout))):\n",
    "                delta = rollout[t][6] + self.gamma * (rollout[t][3] if t + 1 < len(rollout) else 0) - rollout[t][1]\n",
    "                rollout[t].append((delta + self.gamma * self.lam * next_advantage).detach())\n",
    "                next_advantage = rollout[t][8]\n",
    "\n",
    "            for _ in range(K):\n",
    "                samples = random.sample(rollout, bs)\n",
    "                states = torch.stack([s[0] for s in samples])\n",
    "                old_probs = torch.stack([s[5] for s in samples]) # get action probabilites from samples\n",
    "                actions = torch.stack([s[4] for s in samples]) # get selected actions from samples\n",
    "\n",
    "                advantages = torch.tensor([s[8] for s in samples]).to(device)\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                new_probs = torch.stack([self.actor.forward(state).log_prob(action) for state, action in zip(states, actions)]) # get new action probs from sample states\n",
    "                ratio = (new_probs - old_probs).exp()\n",
    "\n",
    "                G_vals = advantages + torch.stack([s[1] for s in samples])\n",
    "\n",
    "                critic_loss = ((G_vals - self.critic.forward(states)) ** 2).mean() # loss for critic network\n",
    "                actor_loss = -torch.min(ratio * advantages, torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages).mean()\n",
    "                self.update_nets(actor_loss, critic_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1f3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode = 'human')\n",
    "agent = Agent.load_agent(env, 'lunarlander_agent.pt', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb8c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation reward: -221.94907544867607\n",
      "evaluation reward: -210.93529436070008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         dist \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mactor(state_t)\n\u001b[1;32m     12\u001b[0m         action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39margmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m         state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_reward)\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    662\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/projects/RL/venv/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:778\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    777\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_rewards = 0\n",
    "eval_episodes = 10\n",
    "for i in range(eval_episodes):\n",
    "    done, truncated = False, False\n",
    "    state, _ = agent.env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (done or truncated):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            dist = agent.actor(state_t)\n",
    "            action = dist.probs.argmax(dim = 1)\n",
    "            state, reward, done, truncated, _ = agent.env.step(action.item())\n",
    "            total_reward += reward\n",
    "\n",
    "    print(\"evaluation reward:\", total_reward)\n",
    "    eval_rewards += total_reward\n",
    "\n",
    "print('average reward:', eval_rewards / eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abdee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b0576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
