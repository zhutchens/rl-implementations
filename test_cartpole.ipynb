{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as opt\n",
    "import random\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9183fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(Actor, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, states: torch.tensor) -> torch.tensor:\n",
    "        logits = self.layers(states)\n",
    "        return Categorical(logits = logits)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,  in_features: int, out_features: int):\n",
    "        super(Critic, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, states: torch.tensor) -> torch.tensor:\n",
    "        return self.layers(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, actor: Actor, critic: Critic, epsilon: float, gamma: float, lam: float, actor_lr: float, critic_lr: float):\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_opt = opt.Adam(self.actor.parameters(), actor_lr)\n",
    "        self.critic_opt = opt.Adam(self.critic.parameters(), critic_lr)\n",
    "\n",
    "    def save_agent(self, path: str = 'cartpole_agent.pt') -> None:\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'actor_lr': self.actor_lr,\n",
    "                'critic_lr': self.critic_lr,\n",
    "                'epsilon': self.epsilon,\n",
    "                'gamma': self.gamma,\n",
    "                'lam': self.lam,\n",
    "                'actor_in_feats': self.actor.in_features,\n",
    "                'actor_out_feats': self.actor.out_features,\n",
    "                'critic_in_feats': self.critic.in_features,\n",
    "                'critic_out_feats': self.critic.out_features\n",
    "            }\n",
    "        }, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_agent(path: str, env, device: str) -> 'Agent':\n",
    "        chckpt = torch.load(path)\n",
    "\n",
    "        actor = Actor(chckpt['hyperparameters']['actor_in_feats'], chckpt['hyperparameters']['actor_out_feats'])\n",
    "        critic = Critic(chckpt['hyperparameters']['critic_in_feats'], chckpt['hyperparameters']['critic_out_feats'])\n",
    "        actor.load_state_dict(chckpt['actor'])\n",
    "        critic.load_state_dict(chckpt['critic'])\n",
    "\n",
    "        return Agent(\n",
    "            env, \n",
    "            actor.to(device), \n",
    "            critic.to(device), \n",
    "            chckpt['hyperparameters']['epsilon'],\n",
    "            chckpt['hyperparameters']['gamma'],\n",
    "            chckpt['hyperparameters']['lam'],\n",
    "            chckpt['hyperparameters']['actor_lr'],\n",
    "            chckpt['hyperparameters']['critic_lr']\n",
    "        )\n",
    "\n",
    "\n",
    "    def select_action(self, states: torch.tensor) -> tuple:\n",
    "        dist = self.actor.forward(states)\n",
    "        actions = dist.sample()\n",
    "\n",
    "        return actions, dist.log_prob(actions)  \n",
    "    \n",
    "    def get_state_values(self, states: torch.tensor) -> torch.tensor:\n",
    "        return self.critic.forward(states)\n",
    "\n",
    "    def update_nets(self, actor_loss: torch.tensor, critic_loss: torch.tensor) -> None:\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "\n",
    "    def fit(self, train_iters: int, timesteps: int, K: int, bs: int) -> None:\n",
    "        if bs > self.env.num_envs * timesteps:\n",
    "            raise ValueError('batch size cannot be greater than number of environments * timesteps')\n",
    "        \n",
    "        self.all_rewards = []\n",
    "        self.all_steps = []\n",
    "\n",
    "        for train_iter in range(train_iters):\n",
    "            rollout = []\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            states, _ = self.env.reset()\n",
    "            states = torch.from_numpy(states).to(device)\n",
    "\n",
    "            for _ in range(timesteps):\n",
    "                actions, probs = self.select_action(states)\n",
    "                state_vals = self.get_state_values(states)\n",
    "\n",
    "                next_states, rewards, dones, terminated, _ = self.env.step(actions.detach().cpu().numpy())\n",
    "                next_states = torch.from_numpy(next_states).to(device)\n",
    "                rewards = torch.from_numpy(rewards)\n",
    "                dones = torch.from_numpy(dones)\n",
    "\n",
    "                ep_reward += sum(rewards).item()\n",
    "\n",
    "                next_state_vals = self.get_state_values(next_states)\n",
    "                for s, sv, ns, nsv, a, p, r, d in zip(states, state_vals.detach(), next_states, next_state_vals, actions, probs.detach(), rewards, dones):\n",
    "                    rollout.append([s, sv, ns, nsv, a, p, r, d])\n",
    "\n",
    "                states = next_states\n",
    "                ep_steps += 1\n",
    "            \n",
    "            print('finished episode:', train_iter)\n",
    "            print('total reward:', ep_reward)\n",
    "            print('number of steps:', ep_steps)\n",
    "            print('-' * 15)\n",
    "\n",
    "            self.all_rewards.append(ep_reward)\n",
    "            self.all_steps.append(ep_steps)\n",
    "\n",
    "            # next_advantage = 0\n",
    "            # for t in reversed(range(len(rollout))):\n",
    "            #     delta = rollout[t][6] + self.gamma * (rollout[t][3] if t + 1 < len(rollout) else 0) - rollout[t][1]\n",
    "            #     rollout[t].append((delta + self.gamma * self.lam * next_advantage).detach())\n",
    "            #     next_advantage = rollout[t][8]\n",
    "\n",
    "            for i in range(len(advantages) - 1): # dont go out of bounds\n",
    "                discount = 1\n",
    "                advantage_t = 0\n",
    "                for j in range(i, len(rollout) - 1):\n",
    "                    delta = (rollout[j][6] + self.gamma * rollout[j][3] - rollout[j][1])\n",
    "                    advantage_t += discount * delta\n",
    "                rollout[i].append(advantage_t)\n",
    "                discount *= self.gamma * self.lam\n",
    "\n",
    "            for _ in range(K):\n",
    "                samples = random.sample(rollout, bs)\n",
    "                states = torch.stack([s[0] for s in samples])\n",
    "                old_probs = torch.stack([s[5] for s in samples]) # get action probabilites from samples\n",
    "                actions = torch.stack([s[4] for s in samples]) # get selected actions from samples\n",
    "\n",
    "                advantages = torch.tensor([s[8] for s in samples]).to(device)\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                new_probs = torch.stack([self.actor.forward(state).log_prob(action) for state, action in zip(states, actions)]) # get new action probs from sample states\n",
    "                ratio = (new_probs - old_probs).exp()\n",
    "\n",
    "                G_vals = advantages + torch.stack([s[1] for s in samples])\n",
    "\n",
    "                critic_loss = ((G_vals - self.critic.forward(states)) ** 2).mean() # loss for critic network\n",
    "                actor_loss = -torch.min(ratio * advantages, torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages).mean()\n",
    "                self.update_nets(actor_loss, critic_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1f3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "agent = Agent.load_agent('cartpole_agent.pt', env, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "evaluation reward: 500.0\n",
      "average reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "eval_rewards = 0\n",
    "eval_episodes = 10\n",
    "for i in range(eval_episodes):\n",
    "    done, truncated = False, False\n",
    "    state, _ = agent.env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (done or truncated):\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            dist = agent.actor(state_t)\n",
    "            action = dist.probs.argmax(dim = 1)\n",
    "            state, reward, done, truncated, _ = agent.env.step(action.item())\n",
    "            total_reward += reward\n",
    "\n",
    "    print(\"evaluation reward:\", total_reward)\n",
    "    eval_rewards += total_reward\n",
    "\n",
    "print('average reward:', eval_rewards / eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abdee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b0576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
